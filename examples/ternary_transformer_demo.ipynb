{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tiny Transformer Quantization Demo\n",
        "Show how to quantize a mini-GPT stack, run generation, and highlight how packed ternary GEMMs shrink memory while surfacing hardware-ready chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "```bash\n",
        "pip install .[torch]\n",
        "```\n",
        "These notebooks assume `t81lib` is installed from the repository so the Python bindings line up with the C++ core.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import t81lib\n",
        "\n",
        "plt.style.use('seaborn-v0_8-colorblind')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model structure\n",
        "We build a micro transformer (64-dimensional embeddings, 2 blocks, 4 heads) and reuse the same linear layers for both float and ternary-inference paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tryte_table():\n",
        "    table = np.empty(27, dtype=np.uint8)\n",
        "    for t2 in (-1, 0, 1):\n",
        "        for t1 in (-1, 0, 1):\n",
        "            for t0 in (-1, 0, 1):\n",
        "                idx = (t0 + 1) + 3 * (t1 + 1) + 9 * (t2 + 1)\n",
        "                value = (t0 + 3 * t1 + 9 * t2) + 13\n",
        "                table[idx] = value\n",
        "    return table\n",
        "\n",
        "TRITS_TO_TRYTE = build_tryte_table()\n",
        "\n",
        "def pack_columnwise(trits: np.ndarray, k_limbs: int, k_actual: int) -> np.ndarray:\n",
        "    cols = trits.shape[1]\n",
        "    padded = np.zeros((k_limbs * 48, cols), dtype=np.int8)\n",
        "    padded[:k_actual, :] = trits[:k_actual, :]\n",
        "    packed = np.zeros((k_limbs, cols), dtype=np.dtype('V16'))\n",
        "    view = packed.view(np.uint8).reshape(-1, 16)\n",
        "    cursor = 0\n",
        "    for limb_idx in range(k_limbs):\n",
        "        chunk = padded[limb_idx * 48 : (limb_idx + 1) * 48]\n",
        "        triples = chunk.reshape(16, 3, cols)\n",
                "        for col in range(cols):\n",
                "            for tryte_idx in range(16):\n",
                "                triple = triples[tryte_idx, :, col] + 1\n",
                "                tri_index = triple[0] + 3 * triple[1] + 9 * triple[2]\n",
                "                view[cursor, tryte_idx] = TRITS_TO_TRYTE[tri_index]\n",
        "            cursor += 1\n",
        "    return packed\n",
        "\n",
        "class QuantizedLinearInference:\n",
        "    def __init__(self, module: nn.Linear, threshold: float = 0.33):\n",
        "        weight = module.weight.detach().cpu().numpy().astype(np.float32)\n",
        "        self.weight_packed = t81lib.pack_dense_matrix(weight, threshold=threshold)\n",
        "        self.k_limbs = self.weight_packed.shape[1]\n",
        "        self.k_actual = weight.shape[1]\n",
        "        self.rows = weight.shape[0]\n",
        "        self.bias = module.bias.detach().cpu().numpy() if module.bias is not None else None\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        batch = inputs.shape[0] * inputs.shape[1] if inputs.ndim == 3 else inputs.shape[0]\n",
        "        flat = inputs.reshape(-1, self.k_actual).detach().cpu().numpy().astype(np.float32)\n",
        "        trits = t81lib.quantize_to_trits(flat, threshold=self.threshold)\n",
        "        packed_rhs = pack_columnwise(trits.T, self.k_limbs, self.k_actual)\n",
        "        output = np.zeros((self.rows * flat.shape[0],), dtype=np.float32)\n",
        "        t81lib.gemm_ternary(\n",
        "            self.weight_packed,\n",
        "            packed_rhs,\n",
        "            output,\n",
        "            self.rows,\n",
        "            flat.shape[0],\n",
        "            self.k_limbs * 48,\n",
        "        )\n",
        "        output = output.reshape(self.rows, flat.shape[0]).T\n",
        "        if self.bias is not None:\n",
        "            output += self.bias.reshape(1, -1)\n",
        "        return torch.from_numpy(output.reshape(inputs.shape[0], inputs.shape[1], self.rows) if inputs.ndim == 3 else output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(hidden_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(F.gelu(self.lin1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.qkv_proj = nn.Linear(dim, dim * 3)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.ffn = FeedForward(dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.heads = heads\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv_proj(x).view(B, T, 3, self.heads, C // self.heads)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(C // self.heads)\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "        attn_out = (attn @ v).reshape(B, T, C)\n",
        "        x = self.norm1(residual + self.out_proj(attn_out))\n",
        "        x = self.norm2(x + self.ffn(x))\n",
        "        return x\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size=64, dim=64, heads=4, layers=2, ff_dim=256, seq_len=16):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(seq_len, dim))\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(dim, heads, ff_dim) for _ in range(layers)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        B, T = tokens.shape\n",
        "        x = self.token_emb(tokens) + self.pos_emb[:T]\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        logits = self.head(self.norm(x))\n",
        "        return logits\n",
        "\n",
        "model = MiniTransformer()\n",
        "model.eval()\n",
        "print('Mini transformer architecture ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_tokens = torch.randint(0, 64, (2, 16))\n",
        "float_logits = model(batch_tokens)\n",
        "print('Float logits shape', float_logits.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.33\n",
        "quantized_layers = {}\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        quantized_layers[name] = QuantizedLinearInference(module, threshold=threshold)\n",
        "print('Quantized linear map entry count', len(quantized_layers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lookup_quantized(name):\n",
        "    return quantized_layers[name]\n",
        "\n",
        "def quantized_forward(tokens: torch.Tensor) -> torch.Tensor:\n",
        "    B, T = tokens.shape\n",
        "    x = model.token_emb(tokens) + model.pos_emb[:T]\n",
        "    for i, block in enumerate(model.blocks):\n",
        "        residual = x\n",
        "        qkv = lookup_quantized(f'blocks.{i}.qkv_proj').forward(x)\n",
        "        qkv = qkv.view(B, T, 3, block.heads, block.dim // block.heads)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(block.dim // block.heads)\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "        attn_out = (attn @ v).reshape(B, T, block.dim)\n",
        "        projected = lookup_quantized(f'blocks.{i}.out_proj').forward(attn_out)\n",
        "        x = block.norm1(residual + projected.reshape(B, T, block.dim))\n",
        "        ffn = lookup_quantized(f'blocks.{i}.ffn.lin1').forward(x)\n",
        "        ffn = F.gelu(ffn)\n",
        "        ffn_out = lookup_quantized(f'blocks.{i}.ffn.lin2').forward(ffn)\n",
        "        x = block.norm2(x + ffn_out)\n",
        "    logits = lookup_quantized('head').forward(model.norm(x))\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_sequence(prefix, forward_fn, steps=10):\n",
        "    ids = list(prefix)\n",
        "    for _ in range(steps):\n",
        "        tokens = torch.tensor([ids], dtype=torch.long)\n",
        "        logits = forward_fn(tokens)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "        ids.append(next_token)\n",
        "    return ids\n",
        "\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz '# 26 lowercase + space\n",
        "prefix = [0, 1, 2, 3]\n",
        "float_seq = sample_sequence(prefix, lambda tokens: model(tokens), steps=6)\n",
        "quant_seq = sample_sequence(prefix, quantized_forward, steps=6)\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join(alphabet[i % len(alphabet)] for i in seq)\n",
        "\n",
        "print('Float generation', decode(float_seq))\n",
        "print('Ternary generation', decode(quant_seq))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def timed_forward(fn, steps=5):\n",
        "    tokens = torch.randint(0, 64, (2, 16))\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(steps):\n",
        "        fn(tokens)\n",
        "    return (time.perf_counter() - start) / steps\n",
        "\n",
        "float_latency = timed_forward(lambda tokens: model(tokens))\n",
        "ternary_latency = timed_forward(quantized_forward)\n",
        "print(f'Float latency {float_latency:.4f}s, ternary latency {ternary_latency:.4f}s (per call)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "float_bytes = sum(m.weight.numel() * 4 for m in model.modules() if isinstance(m, nn.Linear))\n",
        "ternary_bytes = sum(layer.weight_packed.nbytes for layer in quantized_layers.values())\n",
        "binary_bits = sum(m.weight.numel() for m in model.modules() if isinstance(m, nn.Linear))\n",
        "binary_bytes = math.ceil(binary_bits / 8)\n",
        "print(f'Float linear params {float_bytes / 1024:.1f}kB, ternary packed {ternary_bytes / 1024:.1f}kB (~{ternary_bytes / binary_bytes * 100:.1f}% of 1-bit)')\n",
        "\n",
        "labels = ['Float32', 'Ternary (pack)']\n",
        "size_vals = [float_bytes, ternary_bytes]\n",
        "latency_vals = [float_latency, ternary_latency]\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax1.bar(labels, [val / 1024 for val in size_vals], color=['#1f77b4', '#ff7f0e'])\n",
        "ax1.set_ylabel('Memory (kB)')\n",
        "ax1.set_title('Linear layer footprint')\n",
        "ax2.bar(labels, latency_vals, color=['#2ca02c', '#d62728'])\n",
        "ax2.set_ylabel('Latency (s)')\n",
        "ax2.set_title('Per forward call (averaged)')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "values = [1, -1, 1]\n",
        "row_idx = [0, 0, 1]\n",
        "col_idx = [0, 1, 2]\n",
        "B = np.random.rand(3, 8).astype(np.float32)\n",
        "C = np.zeros((2 * 8,), dtype=np.float32)\n",
        "t81lib.spmm_simple(values, row_idx, col_idx, 2, 3, B, C, 8)\n",
        "C = C.reshape(2, 8)\n",
        "print('Sparse attention replay result', C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection and extensibility\n",
        "Packed ternary GEMMs run inside the same training-friendly PyTorch graph but keep the inference kernels deterministic and portable.\n",
        "Use `QuantizedLinearInference` for each projection in larger models (blending through `t81.torch` or by caching quantized layers) and hook into `t81lib.gemm_ternary` + `spmm_simple` to test custom sparsity budgets or simulated accelerator fabrics.\n",
        "To scale to Llama-style stacks, quantize projection weights once, pack them into `pack_dense_matrix`, and schedule the packed buffers to your triplet-aware GEMM engine; converting your existing float checkpoints before conversion keeps evaluation reproducible.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
