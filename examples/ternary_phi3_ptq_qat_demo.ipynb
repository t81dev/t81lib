{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-3-mini PTQ + QAT ternary demo\n",
    "\n",
    "This notebook shows a compact, reproducible workflow for measuring balanced ternary compression\n",
    "and latency wins on a real LLM checkpoint. It uses a CPU-first path by default, with optional\n",
    "GPU acceleration if available.\n",
    "\n",
    "**Model:** `microsoft/Phi-3-mini-4k-instruct` (3.8B params)\n",
    "**Goals:**\n",
    "- Measure FP16/FP32 baseline size, perplexity, and latency.\n",
    "- Apply PTQ via `t81` and compare size + speed.\n",
    "- (Optional) run a short QAT loop for quality retention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary (fill after a run)\n",
    "\n",
    "| Mode | Size (GiB) | Compression | Perplexity | Tok/s |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| FP16/FP32 | - | - | - | - |\n",
    "| PTQ (ternary) | - | - | - | - |\n",
    "| QAT (ternary) | - | - | - | - |\n",
    "\n",
    "Phi-3 GGUF benchmark (llama.cpp, TQ1_0, CPU-only):\n",
    "\n",
    "| Bundle | Size (MiB) | Peak RSS (MiB) | Prompt ms/token | Prompt tok/s | Eval ms/token | Eval tok/s |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| phi3-tq1-fixed12.gguf | 1481.96 | 2260.02 | 54.35 | 18.4 | 56.22 | 17.79 |\n",
    "\n",
    "Small-model baseline (GPT-2, CPU, tiny eval/QAT):\n",
    "\n",
    "| Mode | Size (GiB) | Compression | Perplexity | Tok/s |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| FP16/FP32 | 0.46 | - | 163.64 | 34.81 |\n",
    "| PTQ (ternary) | 0.01 | 38.7x | nan | 0.04 |\n",
    "| QAT (ternary) | 0.01 | 38.7x | nan | 0.07 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies (run once):\n",
    "\n",
    "```bash\n",
    "pip install \".[torch]\" transformers datasets\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- CPU runs are supported on any machine, but expect longer runtimes.\n",
    "- If you have a GPU, the same cells will run faster automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "import t81 as t8\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "THRESHOLD = 0.45\n",
    "MAX_EVAL_TOKENS = 2048\n",
    "MAX_TOKENS_GENERATE = 128\n",
    "RUN_QAT = False  # Set True to run a short QAT loop.\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"Device: {DEVICE}, dtype: {DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bytes_to_gib(num_bytes: int) -> float:\n",
    "    return num_bytes / (1024 ** 3)\n",
    "\n",
    "\n",
    "def model_param_bytes(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "\n",
    "def ternary_weight_bytes(model: torch.nn.Module) -> int:\n",
    "    total = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, t8.Linear):\n",
    "            rows, cols = module.weight.shape\n",
    "            k_limbs = (cols + 47) // 48\n",
    "            total += rows * k_limbs * 16\n",
    "            if module.bias is not None:\n",
    "                total += module.bias.numel() * module.bias.element_size()\n",
    "    return total\n",
    "\n",
    "\n",
    "def measure_generate_latency(model, tokenizer, prompt: str, max_new_tokens: int) -> dict:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    _ = model.generate(**inputs, max_new_tokens=1)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start = time.perf_counter()\n",
    "    _ = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    elapsed = time.perf_counter() - start\n",
    "    tokens = max_new_tokens\n",
    "    return {\"seconds\": elapsed, \"tok_per_sec\": tokens / max(elapsed, 1e-6)}\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def perplexity(model, tokenizer, dataset, max_tokens: int) -> float:\n",
    "    model.eval()\n",
    "    text = \"\\n\\n\".join(dataset[\"text\"])\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"][0][:max_tokens]\n",
    "    input_ids = input_ids.unsqueeze(0).to(next(model.parameters()).device)\n",
    "    labels = input_ids.clone()\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    return torch.exp(outputs.loss).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else \"cpu\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_bytes = model_param_bytes(model)\n",
    "print(f\"Baseline param size: {bytes_to_gib(baseline_bytes):.2f} GiB\")\n",
    "\n",
    "prompt = \"Summarize balanced ternary quantization in one paragraph.\"\n",
    "latency = measure_generate_latency(model, tokenizer, prompt, MAX_TOKENS_GENERATE)\n",
    "print(f\"Baseline latency: {latency['tok_per_sec']:.2f} tok/s\")\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "baseline_ppl = perplexity(model, tokenizer, wikitext, MAX_EVAL_TOKENS)\n",
    "print(f\"Baseline perplexity: {baseline_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ: swap in ternary layers\n",
    "\n",
    "`t81.convert` replaces every `nn.Linear` with `t81.nn.Linear` and caches packed ternary weights\n",
    "on first inference, so you can measure size and latency immediately.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = t8.convert(model, threshold=THRESHOLD, inplace=True)\n",
    "model.eval()\n",
    "\n",
    "ternary_bytes = ternary_weight_bytes(model)\n",
    "print(f\"Ternary packed size (weights + biases): {bytes_to_gib(ternary_bytes):.2f} GiB\")\n",
    "print(f\"Compression ratio: {baseline_bytes / max(ternary_bytes, 1):.1f}x\")\n",
    "\n",
    "latency_ptq = measure_generate_latency(model, tokenizer, prompt, MAX_TOKENS_GENERATE)\n",
    "print(f\"PTQ latency: {latency_ptq['tok_per_sec']:.2f} tok/s\")\n",
    "\n",
    "ptq_ppl = perplexity(model, tokenizer, wikitext, MAX_EVAL_TOKENS)\n",
    "print(f\"PTQ perplexity: {ptq_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional QAT (short run)\n",
    "\n",
    "Set `RUN_QAT = True` to run a short QAT loop on a small slice of Wikitext-2.\n",
    "This is intentionally small to keep runtimes manageable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if RUN_QAT:\n",
    "    train_split = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "    tokenized = train_split.map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    args = t8.TernaryTrainingArguments(\n",
    "        output_dir=\"phi3-qat\",\n",
    "        per_device_train_batch_size=1,\n",
    "        max_steps=100,\n",
    "        logging_steps=20,\n",
    "        save_steps=100,\n",
    "        learning_rate=5e-5,\n",
    "        ternary_threshold=THRESHOLD,\n",
    "        ternary_warmup_steps=50,\n",
    "    )\n",
    "\n",
    "    trainer = t8.TernaryTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    qat_ppl = perplexity(model, tokenizer, wikitext, MAX_EVAL_TOKENS)\n",
    "    latency_qat = measure_generate_latency(model, tokenizer, prompt, MAX_TOKENS_GENERATE)\n",
    "\n",
    "    print(f\"QAT perplexity: {qat_ppl:.2f}\")\n",
    "    print(f\"QAT latency: {latency_qat['tok_per_sec']:.2f} tok/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary table\n",
    "\n",
    "Fill in the values (or capture them programmatically) to compare baseline vs PTQ vs QAT.\n",
    "\n",
    "| Mode | Size (GiB) | Compression | Perplexity | Tok/s |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| FP16/FP32 | - | - | - | - |\n",
    "| PTQ (ternary) | - | - | - | - |\n",
    "| QAT (ternary) | - | - | - | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI equivalent\n",
    "\n",
    "You can reproduce the PTQ conversion + GGUF export via CLI:\n",
    "\n",
    "```bash\n",
    "t81 convert microsoft/Phi-3-mini-4k-instruct phi3-t81 --threshold 0.45 --force-cpu-device-map\n",
    "\n",
    "t81 gguf phi3-tq1.gguf --from-t81 phi3-t81 --quant TQ1_0 --validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI follow-up\n",
    "\n",
    "Use the unified CLI to export a GGUF bundle or inspect available flags:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: inspect CLI helpers\n",
    "!t81 --help\n",
    "\n",
    "# Convert + export GGUF (adjust paths as needed)\n",
    "!t81 convert microsoft/Phi-3-mini-4k-instruct phi3-t81 --threshold 0.45 --force-cpu-device-map\n",
    "!t81 gguf phi3-tq1.gguf --from-t81 phi3-t81 --quant TQ1_0 --validate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}