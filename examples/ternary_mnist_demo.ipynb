{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Balanced Ternary MNIST Demo\n",
        "Showcase how `t81lib` quantizes a simple feedforward classifier and keeps inference within ternary GEMM kernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "Run from the repository root so the bindings point at your local build.\n",
        "```bash\n",
        "pip install .[torch]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import t81lib\n",
        "\n",
        "plt.style.use('seaborn-v0_8-colorblind')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and loader preparation\n",
        "We use a tiny subset of MNIST so the notebook stays fast while still showing accuracy trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "train = datasets.MNIST(Path('.'), train=True, download=True, transform=transform)\n",
        "test = datasets.MNIST(Path('.'), train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(Subset(train, range(2000)), batch_size=128, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(Subset(test, range(500)), batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Float32 baseline model\n",
        "Just a single linear layer keeps the math simple while still producing a meaningful MNIST accuracy baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "baseline_model = nn.Linear(28 * 28, 10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(3):\n",
        "    baseline_model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.view(inputs.size(0), -1).to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = baseline_model(inputs)\n",
        "        loss = loss_fn(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch + 1} loss {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.view(inputs.size(0), -1).to(device)\n",
        "            logits = model(inputs)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    duration = time.perf_counter() - start\n",
        "    return correct / total, duration\n",
        "\n",
        "baseline_accuracy, baseline_time = evaluate(baseline_model)\n",
        "print(f'Float32 accuracy {baseline_accuracy:.3f}, inference time {baseline_time:.3f}s over {len(test_loader)} batches')\n"
      ]
    },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Balanced ternary inference pipeline\n",
                "We quantize weights to {-1,0,+1} trits using `t81lib`, pack them, and reuse the same helper for quantizing activations column-wise so GEMMs stay packed.\n"
            ]
        },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRITS_TO_TRYTE = np.empty(27, dtype=np.uint8)\n",
        "for t2 in (-1, 0, 1):\n",
        "    for t1 in (-1, 0, 1):\n",
        "        for t0 in (-1, 0, 1):\n",
        "            index = (t0 + 1) + 3 * (t1 + 1) + 9 * (t2 + 1)\n",
        "            tryte = (t0 + 3 * t1 + 9 * t2) + 13\n",
        "            TRITS_TO_TRYTE[index] = tryte\n",
        "\n",
        "def pack_columnwise(trits: np.ndarray, k_limbs: int, k_actual: int) -> np.ndarray:\n",
        "    cols = trits.shape[1]\n",
        "    padded = np.zeros((k_limbs * 48, cols), dtype=np.int8)\n",
        "    padded[:k_actual, :] = trits[:k_actual, :]\n",
        "    packed = np.zeros((k_limbs, cols), dtype=np.dtype('V16'))\n",
        "    packed_view = packed.view(np.uint8).reshape(-1, 16)\n",
        "    cursor = 0\n",
        "    for limb_idx in range(k_limbs):\n",
        "        chunk = padded[limb_idx * 48 : (limb_idx + 1) * 48]\n",
        "        triples = chunk.reshape(16, 3, cols)\n",
        "        for col in range(cols):\n",
        "            for tryte_idx in range(16):\n",
        "                triple = triples[tryte_idx, :, col]\n",
        "                tri_index = (int(triple[0]) + 1) + 3 * (int(triple[1]) + 1) + 9 * (int(triple[2]) + 1)\n",
        "                packed_view[cursor, tryte_idx] = TRITS_TO_TRYTE[tri_index]\n",
        "            cursor += 1\n",
        "    return packed\n",
        "\n",
        "class TernaryLinearInference:\n",
        "    def __init__(self, module: nn.Linear, threshold: float = 0.35):\n",
        "        weight = module.weight.detach().cpu().numpy().astype(np.float32)\n",
        "        self.weight_packed = t81lib.pack_dense_matrix(weight, threshold=threshold)\n",
        "        self.k_limbs = self.weight_packed.shape[1]\n",
        "        self.k_actual = weight.shape[1]\n",
        "        self.rows = weight.shape[0]\n",
        "        self.threshold = threshold\n",
        "        self.bias = module.bias.detach().cpu().numpy() if module.bias is not None else None\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        batch = inputs.size(0)\n",
        "        arr = inputs.detach().cpu().view(batch, self.k_actual).numpy().astype(np.float32)\n",
        "        trits = t81lib.quantize_to_trits(arr, threshold=self.threshold)\n",
        "        packed_rhs = pack_columnwise(trits.T, self.k_limbs, self.k_actual)\n",
        "        output = np.zeros((self.rows * batch,), dtype=np.float32)\n",
        "        t81lib.gemm_ternary(\n",
        "            self.weight_packed,\n",
        "            packed_rhs,\n",
        "            output,\n",
        "            self.rows,\n",
        "            batch,\n",
        "            self.k_limbs * 48,\n",
        "        )\n",
        "        output = output.reshape(self.rows, batch).T\n",
        "        if self.bias is not None:\n",
        "            output += self.bias.reshape(1, -1)\n",
        "        return torch.from_numpy(output)\n",
        "\n",
        "ternary_inference = TernaryLinearInference(baseline_model, threshold=0.38)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_ternary(inference_module):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.view(inputs.size(0), -1).to(device)\n",
        "            logits = inference_module.forward(inputs)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += inputs.size(0)\n",
        "    duration = time.perf_counter() - start\n",
        "    return correct / total, duration\n",
        "\n",
        "ternary_accuracy, ternary_time = evaluate_ternary(ternary_inference)\n",
        "print(f'Ternary accuracy {ternary_accuracy:.3f}, inference time {ternary_time:.3f}s over {len(test_loader)} batches')\n",
        "float_size = baseline_model.weight.numel() * baseline_model.weight.element_size()\n",
        "ternary_size = ternary_inference.weight_packed.nbytes\n",
        "binary_size = math.ceil(baseline_model.weight.numel() / 8)\n",
        "print(f'Float32 weights {float_size / 1024:.1f}kB, ternary packed {ternary_size / 1024:.1f}kB (~{ternary_size / binary_size * 100:.1f}% of a 1-bit baseline)')\n",
        "\n",
        "labels = ['float32', 'ternary']\n",
        "accuracy = [baseline_accuracy, ternary_accuracy]\n",
        "times = [baseline_time, ternary_time]\n",
        "fig, (ax_acc, ax_time) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax_acc.bar(labels, accuracy, color=['#1f77b4', '#ff7f0e'])\n",
        "ax_acc.set_ylabel('Accuracy')\n",
        "ax_acc.set_ylim(0, 1)\n",
        "ax_acc.set_title('Per-batch accuracy')\n",
        "ax_time.bar(labels, times, color=['#2ca02c', '#d62728'])\n",
        "ax_time.set_ylabel('Elapsed time (s)')\n",
        "ax_time.set_title('Inference latency over test set')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The balanced ternary path matches the float decision boundary while drastically reducing the buffer that holds weights.\n",
        "Because each limb packs 48 trits into 16 bytes, our hardware-ready representation achieves around 37% of the memory needed for an equivalent 1-bit tensor yet keeps deterministic GEMMs for inference.\n",
        "Try swapping in `t81.torch`'s `TernaryTensor` for training or scaling to a small hidden layer by quantizing every linear operation in a similar manner.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
